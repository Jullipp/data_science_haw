---
title: "R Notebook"
output: html_notebook
---
## Notebook zu Session 4

# Libraries und Daten
```{r}
library(tidyverse)
library(e1071)
library(caret)
library(pROC)
library(rpart)
library(rpart.plot)
```

```{r}
titanic <- read_delim("titanic.csv", ";", 
    escape_double = FALSE, trim_ws = TRUE)
```

# Class Imbalance Check

```{r}
titanic %>%
  group_by(survived) %>%
  summarize(n = n())
```

# SVM

```{r}
(titanic.df <- titanic %>%
  select(survived,pclass,sex,age,sibsp,parch) %>%
   mutate(survived = as.factor(survived)))
```

```{r}
titanic.df <- titanic.df %>%
  mutate(age = as.numeric(str_replace(age,",","."),
         sex = ifelse(sex == "female", 1, 0))) %>%
  na.omit(titanic.df)
```

```{r}
set.seed(393)
inTrain <- createDataPartition(
  y = titanic.df$survived,
  p = .8,
  list = FALSE)
training <- titanic.df[ inTrain,]
testing  <- titanic.df[-inTrain,]
```

```{r}
model.svm <- svm(formula = survived ~ ., data = training, probability=TRUE)
summary(model.svm)
pred <- predict(model.svm, testing[,-1], probability = TRUE)
```

```{r}
(test.results <- cbind(pred, testing))
```
```{r}
head(attr(pred, "probabilities"))
```

```{r}
confusionMatrix(pred,testing$survived)
```


```{r}
pROC_obj <- roc(as.numeric(test.results$survived), as.numeric(test.results$pred),
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
```


# Naive Bayes

```{r}
my_training <- training %>%
  mutate(survived = as.factor(survived))%>%
  mutate(sex = as.factor(sex))%>%
  mutate(pclass = as.factor(pclass)) %>%
  mutate(sibsp = as.factor(sibsp)) %>%
  mutate(parch = as.factor(parch)) %>%
  mutate(age = as.factor(ifelse(age < 16, "child", "adult")))
model <- naiveBayes(survived ~ ., data = my_training)
model
```

```{r}
my_testing <- testing %>%
  mutate(sex = as.factor(sex)) %>%
  mutate(pclass = as.factor(pclass))%>%
  mutate(age = as.factor(ifelse(age < 16, "child", "adult")))%>%
  mutate(sibsp = as.factor(sibsp)) %>%
  mutate(parch = as.factor(parch))
pred <- predict(model, my_testing)
table(pred, my_testing$survived)
```


```{r}
(test.results <- cbind(pred, my_testing))
```

```{r}
test.results <- test.results %>%
  mutate(pred = as.numeric(pred))
pROC_obj <- roc(as.numeric(as.factor(test.results$survived)), test.results$pred,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)

```
# Decision Tree

```{r}
tree<- rpart(survived~., data = training, method = 'class')
rpart.plot(tree)
```
```{r}
dt_results <- predict(tree, testing[,-1], type = 'prob')
head(model.results.dt <- cbind(testing,dt_results),500)
```
```{r}
test.results_dt <- test.results 
table(test.results_dt$pred, testing$survived)
```


```{r}
pROC_obj <- roc(model.results.dt$survived,model.results.dt$`1`,
            smoothed = TRUE,
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
```

# Was sind die Unterschiede in der Performance der Algorithmen und wie lassen sie sich erklären?
Der Descision Tree hat eine Performance von 0.87. Der SVM hat eine Performance von 0.852. Der Naive Bayes hat eine Performance von 0.802.

Die hohe Performance kommt vermutlich daher, dass die Datenpunkte, durch die die Support-Vektoren gehen, weit voneinander getrennt sind. Somit kann der Algorithmus leicter Entscheidungen treffen, da die Trennung zwischen den Möglichkeiten survived oder not survived eindeutiger ist. Der Naive Bayes Algorithmus hat am schlechtesten abgeschnitten, was vermutlich daran lag, 
